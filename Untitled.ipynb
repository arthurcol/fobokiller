{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9226c297",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from selenium import webdriver\n",
    "import matplotlib.pyplot as plt\n",
    "import folium\n",
    "import time\n",
    "from selenium import webdriver\n",
    "import chromedriver_binary\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as ec\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from bs4 import BeautifulSoup\n",
    "from dotenv import load_dotenv, dotenv_values, find_dotenv\n",
    "from fobokiller.utils import subzones_paris\n",
    "from os.path import join,dirname\n",
    "import os\n",
    "import csv\n",
    "\n",
    "env_path = find_dotenv()\n",
    "#env_path = join(dirname(dirname(__file__)), '.env')\n",
    "load_dotenv(env_path)\n",
    "\n",
    "#api_key = dotenv_values()[\"YELP_KEY\"]\n",
    "\n",
    "api_key = os.getenv('YELP_KEY')\n",
    "\n",
    "def get_restaurants(centers, radius):\n",
    "    \"\"\"\n",
    "    Returns DataFrame of restaurants in Paris\n",
    "    \"\"\"\n",
    "    headers = {'Authorization': f'Bearer {api_key}'}\n",
    "    url = 'https://api.yelp.com/v3/businesses/search'\n",
    "\n",
    "    data = []\n",
    "\n",
    "    for i, c in enumerate(centers):\n",
    "        print(f'---------- Requesting API for subzone #{i+1} ----------')\n",
    "        for offset in range(0, 200, 50):\n",
    "            print(\n",
    "                f'   ------- Requesting API with offset = {offset} -------   ')\n",
    "            params = {\n",
    "                'limit': 50,\n",
    "                'categories': ['restaurants'],\n",
    "                'sort_by': 'review_count',\n",
    "                'offset': offset,\n",
    "                'latitude': c[0],\n",
    "                'longitude': c[1],\n",
    "                'radius': int(radius)\n",
    "            }\n",
    "\n",
    "            response = requests.get(url, headers=headers, params=params)\n",
    "            if response.status_code == 200:\n",
    "                data += response.json()['businesses']\n",
    "            elif response.status_code == 400:\n",
    "                print('400 Bad Request')\n",
    "                break\n",
    "\n",
    "    print(f'#####   Request completed, {len(data)} businesses fetched   ###')\n",
    "    return data\n",
    "\n",
    "\n",
    "### Create DF for Yelp data\n",
    "\n",
    "\n",
    "def create_df_yelp(data):\n",
    "\n",
    "    df = pd.DataFrame(columns=[\n",
    "        'alias', 'name', 'url', 'categories', 'latitude', 'longitude',\n",
    "        'address', 'zip_code', 'price', 'rating', 'review_count'\n",
    "    ])\n",
    "\n",
    "    features_to_loop = [\n",
    "        'alias', 'name', 'url', 'categories', 'price', 'rating', 'review_count'\n",
    "    ]\n",
    "\n",
    "    #populate DF\n",
    "    #if condition to avoid raising errors in case restaurant doesn't have all informations\n",
    "\n",
    "    for i, d in enumerate(data):\n",
    "\n",
    "        for f in features_to_loop:\n",
    "            if f in d:\n",
    "                df.loc[i, f] = d[f]\n",
    "            else:\n",
    "                df.loc[i, f] = ''\n",
    "\n",
    "        if 'location' in d:\n",
    "            if 'latitude' in d['coordinates']:\n",
    "                df.loc[i, 'latitude'] = d['coordinates']['latitude']\n",
    "            else:\n",
    "                df.loc[i, 'latitude'] = ''\n",
    "\n",
    "            if 'longitude' in d['coordinates']:\n",
    "                df.loc[i, 'longitude'] = d['coordinates']['longitude']\n",
    "            else:\n",
    "                df.loc[i, 'longitude'] = ''\n",
    "\n",
    "            if 'address1' in d['location']:\n",
    "                df.loc[i, 'address'] = d['location']['address1']\n",
    "            else:\n",
    "                df.loc[i, 'address'] = ''\n",
    "\n",
    "            if 'zip_code' in d['location']:\n",
    "                df.loc[i, 'zip_code'] = d['location']['zip_code']\n",
    "            else:\n",
    "                df.loc[i, 'zip_code'] = 0\n",
    "\n",
    "    #clean DF\n",
    "    #dtypes\n",
    "    df['latitude'] = df['latitude'].astype(float)\n",
    "    df['longitude'] = df['longitude'].astype(float)\n",
    "    df['zip_code'] = df['zip_code'].replace('', 0).astype(int)\n",
    "    df['rating'] = df['rating'].astype(float)\n",
    "    df['review_count'] = df['review_count'].astype(float)\n",
    "\n",
    "    #url\n",
    "    df['url'] = df['url'].apply(lambda txt: txt.split('?', 1)[0])\n",
    "\n",
    "    #price\n",
    "    prices = {'€': '1', '€€': '2', '€€€': '3', '€€€€': '4'}\n",
    "\n",
    "    for euro, num in prices.items():\n",
    "        df['price'] = df['price'].replace(euro, num)\n",
    "\n",
    "    df['price'] = df['price'].replace('', 0).astype(int)\n",
    "\n",
    "    #categories\n",
    "    df['categories'] = df['categories'].apply(\n",
    "        lambda dicts: ', '.join([d['alias'] for d in dicts]))\n",
    "\n",
    "    return df.drop_duplicates()\n",
    "\n",
    "\n",
    "\n",
    "def get_place_google_id(name,latitude,longitude):\n",
    "\n",
    "    url = 'https://maps.googleapis.com/maps/api/place/findplacefromtext/json'\n",
    "    params={\n",
    "        'key' :  os.getenv('GOOGLE_PLACE_KEY'),\n",
    "        'input' : name,\n",
    "        'inputtype' : 'textquery',\n",
    "        'locationbias' : f'point:{latitude},{longitude}'\n",
    "    }\n",
    "\n",
    "    response = requests.get(url,params=params)\n",
    "\n",
    "    #if conditions to avoid raising errors\n",
    "    if response.status_code != 200:\n",
    "        return ''\n",
    "\n",
    "    if 'candidates' in response.json():\n",
    "        response = response.json()['candidates']\n",
    "        if len(response)==0:\n",
    "            return ''\n",
    "        if 'place_id' in response[0]:\n",
    "            return response[0]['place_id']\n",
    "\n",
    "    return ''\n",
    "\n",
    "\n",
    "\n",
    "## Get place url\n",
    "\n",
    "\n",
    "def get_place_google_url(place_id):\n",
    "    url = 'https://maps.googleapis.com/maps/api/place/details/json'\n",
    "    params = {\n",
    "        'key': os.getenv('GOOGLE_PLACE_KEY'),\n",
    "        'place_id': place_id,\n",
    "        'fields': 'url'\n",
    "    }\n",
    "\n",
    "    response = requests.get(url, params=params)\n",
    "\n",
    "    #if conditions to avoid raising errors\n",
    "    if response.status_code != 200:\n",
    "        return ''\n",
    "\n",
    "    if 'result' in response.json():\n",
    "        response = response.json()['result']\n",
    "        if 'url' in response:\n",
    "            return response['url']\n",
    "\n",
    "    return ''\n",
    "\n",
    "\n",
    "\n",
    "def get_reviews_google(url,scroll_limit=None,quiet_mode=True,return_count=False):\n",
    "    options=Options()\n",
    "    if quiet_mode:\n",
    "        options.add_argument('--headless')\n",
    "    driver = webdriver.Chrome(options=options)\n",
    "    driver.get(url)\n",
    "\n",
    "\n",
    "    ###Expand all the reviews using Selenium\n",
    "    # privacy pop-up\n",
    "    xpath = \"/html/body/c-wiz/div/div/div/div[2]/div[1]/div[4]/form/div[1]/div/button/span\"\n",
    "    driver.find_element_by_xpath(xpath).click()\n",
    "\n",
    "    #review_count click\n",
    "    xpath = '//*[@id=\"pane\"]/div/div[1]/div/div/div[2]/div[1]/div[1]/div[2]/div/div[1]/span[1]/span/span[1]/span[2]'\n",
    "\n",
    "    review_count = driver.find_element_by_xpath(xpath).text\n",
    "    review_count=review_count.split(' ', 1)[0]\n",
    "\n",
    "    driver.find_element_by_xpath(xpath).click()\n",
    "\n",
    "    # check\n",
    "    #driver.find_element_by_xpath(\"/html/body/div[3]/div[9]/div[8]/div/div[1]/div/div/div[38]/div/button/span/span\").click()\n",
    "\n",
    "    #scroll to show all reviews\n",
    "    time.sleep(2)\n",
    "    if scroll_limit:\n",
    "        review_count=scroll_limit\n",
    "    scrollable_div = driver.find_element_by_xpath('//*[@id=\"pane\"]/div/div[1]/div/div/div[2]')\n",
    "    for i in range(0,(round(int(review_count)/10-1))):\n",
    "        driver.execute_script('arguments[0].scrollTop = arguments[0].scrollHeight',\n",
    "                scrollable_div)\n",
    "        time.sleep(2)\n",
    "\n",
    "\n",
    "    ### Scrap the reviews info using BS\n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "\n",
    "    #Scrap the reviews text\n",
    "    reviews_soup = soup.find_all('div', class_='ODSEW-ShBeI NIyLF-haAclf gm2-body-2')\n",
    "    reviews = [r.text for r in reviews_soup]\n",
    "\n",
    "    #Scrap the reviews rate\n",
    "    review_rates_soup = [s.find('span',class_='ODSEW-ShBeI-H1e3jb') for s in reviews_soup]\n",
    "    review_rates = [rr.attrs['aria-label'][1] for rr in review_rates_soup]\n",
    "    #Scrap the reviews date\n",
    "    review_dates_soup=[s.find('span', class_='ODSEW-ShBeI-RgZmSc-date') for s in reviews_soup]\n",
    "    review_dates=[rd.text for rd in review_dates_soup]\n",
    "\n",
    "\n",
    "    if return_count:\n",
    "\n",
    "        return review_count,review_dates,review_rates,reviews\n",
    "\n",
    "\n",
    "    return review_dates,review_rates,reviews\n",
    "\n",
    "### Get all reviews from a Google page\n",
    "\n",
    "\n",
    "def get_reviews_google(url,\n",
    "                       scroll_limit=None,\n",
    "                       quiet_mode=True,\n",
    "                       return_count=False):\n",
    "\n",
    "    # Import the webdriver\n",
    "    options = webdriver.ChromeOptions()\n",
    "    options.add_experimental_option('prefs',\n",
    "                                    {'intl.accept_languages': 'en,en_US'})\n",
    "\n",
    "    if quiet_mode:\n",
    "        options.add_argument('--headless')\n",
    "    driver = webdriver.Chrome(chrome_options=options)\n",
    "    driver.get(url)\n",
    "\n",
    "    # privacy pop-up\n",
    "\n",
    "    xpath = \"/html/body/c-wiz/div/div/div/div[2]/div[1]/div[4]/form/div[1]/div/button/span\"\n",
    "    driver.find_element_by_xpath(xpath).click()\n",
    "\n",
    "    #### expand the review\n",
    "\n",
    "    time.sleep(2)\n",
    "\n",
    "    class_ = \"ODSEW-KoToPc-ShBeI gXqMYb-hSRGPd\"\n",
    "\n",
    "    soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "\n",
    "    #total_number_of_reviews = soup.find(\"div\", class_=\"gm2-caption\").text\n",
    "    total_number_of_reviews = driver.find_element_by_xpath(\n",
    "        '//*[@id=\"pane\"]/div/div[1]/div/div/div[2]/div[1]/div[1]/div[2]/div/div[1]/span[1]/span/span[1]/span[2]'\n",
    "    ).text\n",
    "    total_number_of_reviews = total_number_of_reviews.split(' ', 1)[0]\n",
    "\n",
    "    ## Catch nombre d'avis\n",
    "    xpath = '//*[@id=\"pane\"]/div/div[1]/div/div/div[2]/div[1]/div[1]/div[2]/div/div[1]/span[1]/span/span[1]/span[2]'\n",
    "\n",
    "    review_count = driver.find_element_by_xpath(xpath).text\n",
    "    review_count = review_count.split(' ', 1)[0]\n",
    "\n",
    "    driver.find_element_by_xpath(\n",
    "        '//*[@id=\"pane\"]/div/div[1]/div/div/div[2]/div[1]/div[1]/div[2]/div/div[1]/span[1]/span/span[1]/span[2]'\n",
    "    ).click()\n",
    "    #total_number_of_reviews = soup.find(\"div\", class_=\"gm2-caption\").text\n",
    "    #a = total_number_of_reviews\n",
    "    time.sleep(1)\n",
    "    try :\n",
    "        xpatrier = \"/html/body/div[3]/div[9]/div[8]/div/div[1]/div/div/div[2]/div[7]/div[2]/button/span\"\n",
    "        driver.find_element_by_xpath(xpatrier).click()\n",
    "    except :\n",
    "        pass\n",
    "    time.sleep(2)\n",
    "    try :\n",
    "        xpatrecent = \"/html/body/div[3]/div[3]/div[1]/ul/li[2]\"\n",
    "        driver.find_element_by_xpath(xpatrecent).click()\n",
    "    except :\n",
    "        pass\n",
    "    ## Catch cellule of reviews\n",
    "\n",
    "    books_html = soup.findAll('div', class_=\"siAUzd-neVct\")\n",
    "    len(books_html)\n",
    "\n",
    "    #scroll to show all reviews\n",
    "    time.sleep(2)\n",
    "    if scroll_limit:\n",
    "        review_count = scroll_limit\n",
    "    scrollable_div = driver.find_element_by_xpath(\n",
    "        '//*[@id=\"pane\"]/div/div[1]/div/div/div[2]')\n",
    "    for i in range(0, (round(int(review_count) / 10 - 1))):\n",
    "        driver.execute_script(\n",
    "            'arguments[0].scrollTop = arguments[0].scrollHeight',\n",
    "            scrollable_div)\n",
    "        time.sleep(2)\n",
    "\n",
    "    #Find scroll layout\n",
    "    scrollable_div = driver.find_element_by_xpath(\n",
    "        '//*[@id=\"pane\"]/div/div[1]/div/div/div[2]')\n",
    "\n",
    "    print(review_count)\n",
    "    #Scroll as many times as necessary to load all reviews\n",
    "    for i in range(0, (round(int(review_count / 10 - 1)))):\n",
    "        driver.execute_script(\n",
    "            'arguments[0].scrollTop = arguments[0].scrollHeight',\n",
    "            scrollable_div)\n",
    "        time.sleep(2)\n",
    "\n",
    "    response = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "\n",
    "    reviews = response.find_all('div',\n",
    "                                class_='ODSEW-ShBeI NIyLF-haAclf gm2-body-2')\n",
    "\n",
    "    return reviews\n",
    "\n",
    "\n",
    "def get_review_summary(result_set):\n",
    "    rev_dict = {'Review Rate': [], 'Review Time': [], 'Review Text': []}\n",
    "    for result in result_set:\n",
    "        review_rate = result.find('span',\n",
    "                                  class_='ODSEW-ShBeI-H1e3jb')[\"aria-label\"]\n",
    "        review_time = result.find('span',\n",
    "                                  class_='ODSEW-ShBeI-RgZmSc-date').text\n",
    "        review_text = result.find('span', class_='ODSEW-ShBeI-text').text\n",
    "        rev_dict['Review Rate'].append(review_rate)\n",
    "        rev_dict['Review Time'].append(review_time)\n",
    "        rev_dict['Review Text'].append(review_text)\n",
    "    A = pd.DataFrame(rev_dict)\n",
    "\n",
    "    A[\"Review Rate\"] = [i.split(\"\\xa0\")[0] for i in A[\"Review Rate\"]]\n",
    "    A[\"Review Time\"] = [i.strip(\"il y a \") for i in A[\"Review Time\"]]\n",
    "    A[\"Review Time\"] = [i.replace(\"une\", \"1\") for i in A[\"Review Time\"]]\n",
    "    A[\"Review Time\"] = [i.replace(\"un\", \"1\") for i in A[\"Review Time\"]]\n",
    "    return A\n",
    "\n",
    "\n",
    "def get_all_gr(url, iid, name, alias):\n",
    "    test = get_reviews_google(url, scroll_limit=10, quiet_mode=False)\n",
    "    table = get_review_summary(test)\n",
    "    table[\"id\"] = iid\n",
    "    table[\"name\"] = name\n",
    "    table[\"alias\"] = alias\n",
    "    table.to_csv(name.replace(\" \", \"_\").replace(\"'\", \"\") + \".csv\")\n",
    "    os.system(\"\"\"gsutil cp '*.csv' 'gs://wagon-data-722-manoharan/restaurant/'\"\"\")\n",
    "    return table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a46dbdc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "centers, radius = subzones_paris(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "01459360",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- Requesting API for subzone #1 ----------\n",
      "   ------- Requesting API with offset = 0 -------   \n",
      "   ------- Requesting API with offset = 50 -------   \n",
      "   ------- Requesting API with offset = 100 -------   \n",
      "   ------- Requesting API with offset = 150 -------   \n",
      "#####   Request completed, 200 businesses fetched   ###\n"
     ]
    }
   ],
   "source": [
    "df = get_restaurants(centers, radius)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "783a53e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = create_df_yelp(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6179f87e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.head().copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2a7b0f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"id\"] = df.apply(lambda x: get_place_google_id(x[\"name\"], x[\"latitude\"],\n",
    "                                                      x[\"longitude\"]),\n",
    "                        axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "10c8814f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"lien\"] = df.apply(lambda x: get_place_google_url(x[\"id\"]), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9fb147bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'AIzaSyDATqRkvgk2P1bbIu6itIq2jAuiaPxnhfc'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getenv('GOOGLE_PLACE_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fa8f0bbd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'tjiK2WrTVRJyobBmFkzDltuzbS2vnXfndtwrySrK0QHFpqKgaoK1_0mLOpIJ8b5hgB59zW5nWCv1rYnHcgTI9bZthCKEZQV2PUweLg_QxVNO7JwsDfakFXf-S4mbYXYx'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "api_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbfaf041",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
