{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "9d302abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from selenium import webdriver\n",
    "import matplotlib.pyplot as plt\n",
    "import folium\n",
    "import time\n",
    "from selenium import webdriver\n",
    "import chromedriver_binary\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as ec\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from bs4 import BeautifulSoup\n",
    "from dotenv import load_dotenv, dotenv_values, find_dotenv\n",
    "from fobokiller.utils import subzones_paris\n",
    "from os.path import join,dirname\n",
    "import os\n",
    "import csv\n",
    "\n",
    "env_path = find_dotenv()\n",
    "#env_path = join(dirname(dirname(__file__)), '.env')\n",
    "load_dotenv(env_path)\n",
    "\n",
    "#api_key = dotenv_values()[\"YELP_KEY\"]\n",
    "\n",
    "api_key = os.getenv('YELP_KEY')\n",
    "\n",
    "def get_restaurants(centers, radius):\n",
    "    \"\"\"\n",
    "    Returns DataFrame of restaurants in Paris\n",
    "    \"\"\"\n",
    "    headers = {'Authorization': f'Bearer {api_key}'}\n",
    "    url = 'https://api.yelp.com/v3/businesses/search'\n",
    "\n",
    "    data = []\n",
    "\n",
    "    for i, c in enumerate(centers):\n",
    "        print(f'---------- Requesting API for subzone #{i+1} ----------')\n",
    "        for offset in range(0, 200, 50):\n",
    "            print(\n",
    "                f'   ------- Requesting API with offset = {offset} -------   ')\n",
    "            params = {\n",
    "                'limit': 50,\n",
    "                'categories': ['restaurants'],\n",
    "                'sort_by': 'review_count',\n",
    "                'offset': offset,\n",
    "                'latitude': c[0],\n",
    "                'longitude': c[1],\n",
    "                'radius': int(radius)\n",
    "            }\n",
    "\n",
    "            response = requests.get(url, headers=headers, params=params)\n",
    "            if response.status_code == 200:\n",
    "                data += response.json()['businesses']\n",
    "            elif response.status_code == 400:\n",
    "                print('400 Bad Request')\n",
    "                break\n",
    "\n",
    "    print(f'#####   Request completed, {len(data)} businesses fetched   ###')\n",
    "    return data\n",
    "\n",
    "\n",
    "### Create DF for Yelp data\n",
    "\n",
    "\n",
    "def create_df_yelp(data):\n",
    "\n",
    "    df = pd.DataFrame(columns=[\n",
    "        'alias', 'name', 'url', 'categories', 'latitude', 'longitude',\n",
    "        'address', 'zip_code', 'price', 'rating', 'review_count'\n",
    "    ])\n",
    "\n",
    "    features_to_loop = [\n",
    "        'alias', 'name', 'url', 'categories', 'price', 'rating', 'review_count'\n",
    "    ]\n",
    "\n",
    "    #populate DF\n",
    "    #if condition to avoid raising errors in case restaurant doesn't have all informations\n",
    "\n",
    "    for i, d in enumerate(data):\n",
    "\n",
    "        for f in features_to_loop:\n",
    "            if f in d:\n",
    "                df.loc[i, f] = d[f]\n",
    "            else:\n",
    "                df.loc[i, f] = ''\n",
    "\n",
    "        if 'location' in d:\n",
    "            if 'latitude' in d['coordinates']:\n",
    "                df.loc[i, 'latitude'] = d['coordinates']['latitude']\n",
    "            else:\n",
    "                df.loc[i, 'latitude'] = ''\n",
    "\n",
    "            if 'longitude' in d['coordinates']:\n",
    "                df.loc[i, 'longitude'] = d['coordinates']['longitude']\n",
    "            else:\n",
    "                df.loc[i, 'longitude'] = ''\n",
    "\n",
    "            if 'address1' in d['location']:\n",
    "                df.loc[i, 'address'] = d['location']['address1']\n",
    "            else:\n",
    "                df.loc[i, 'address'] = ''\n",
    "\n",
    "            if 'zip_code' in d['location']:\n",
    "                df.loc[i, 'zip_code'] = d['location']['zip_code']\n",
    "            else:\n",
    "                df.loc[i, 'zip_code'] = 0\n",
    "\n",
    "    #clean DF\n",
    "    #dtypes\n",
    "    df['latitude'] = df['latitude'].astype(float)\n",
    "    df['longitude'] = df['longitude'].astype(float)\n",
    "    df['zip_code'] = df['zip_code'].replace('', 0).astype(int)\n",
    "    df['rating'] = df['rating'].astype(float)\n",
    "    df['review_count'] = df['review_count'].astype(float)\n",
    "\n",
    "    #url\n",
    "    df['url'] = df['url'].apply(lambda txt: txt.split('?', 1)[0])\n",
    "\n",
    "    #price\n",
    "    prices = {'€': '1', '€€': '2', '€€€': '3', '€€€€': '4'}\n",
    "\n",
    "    for euro, num in prices.items():\n",
    "        df['price'] = df['price'].replace(euro, num)\n",
    "\n",
    "    df['price'] = df['price'].replace('', 0).astype(int)\n",
    "\n",
    "    #categories\n",
    "    df['categories'] = df['categories'].apply(\n",
    "        lambda dicts: ', '.join([d['alias'] for d in dicts]))\n",
    "\n",
    "    return df.drop_duplicates()\n",
    "\n",
    "\n",
    "\n",
    "def get_place_google_id(name,latitude,longitude):\n",
    "\n",
    "    url = 'https://maps.googleapis.com/maps/api/place/findplacefromtext/json'\n",
    "    params={\n",
    "        'key' :  os.getenv('GOOGLE_PLACE_KEY'),\n",
    "        'input' : name,\n",
    "        'inputtype' : 'textquery',\n",
    "        'locationbias' : f'point:{latitude},{longitude}'\n",
    "    }\n",
    "\n",
    "    response = requests.get(url,params=params)\n",
    "\n",
    "    #if conditions to avoid raising errors\n",
    "    if response.status_code != 200:\n",
    "        return ''\n",
    "\n",
    "    if 'candidates' in response.json():\n",
    "        response = response.json()['candidates']\n",
    "        if len(response)==0:\n",
    "            return ''\n",
    "        if 'place_id' in response[0]:\n",
    "            return response[0]['place_id']\n",
    "\n",
    "    return ''\n",
    "\n",
    "\n",
    "\n",
    "## Get place url\n",
    "\n",
    "\n",
    "def get_place_google_url(place_id):\n",
    "    url = 'https://maps.googleapis.com/maps/api/place/details/json'\n",
    "    params = {\n",
    "        'key': os.getenv('GOOGLE_PLACE_KEY'),\n",
    "        'place_id': place_id,\n",
    "        'fields': 'url'\n",
    "    }\n",
    "\n",
    "    response = requests.get(url, params=params)\n",
    "\n",
    "    #if conditions to avoid raising errors\n",
    "    if response.status_code != 200:\n",
    "        return ''\n",
    "\n",
    "    if 'result' in response.json():\n",
    "        response = response.json()['result']\n",
    "        if 'url' in response:\n",
    "            return response['url']\n",
    "\n",
    "    return ''\n",
    "\n",
    "\n",
    "\n",
    "def get_reviews_google(url,scroll_limit=None,quiet_mode=True,return_count=False):\n",
    "    options=Options()\n",
    "    if quiet_mode:\n",
    "        options.add_argument('--headless')\n",
    "    driver = webdriver.Chrome(options=options)\n",
    "    driver.get(url)\n",
    "\n",
    "\n",
    "    ###Expand all the reviews using Selenium\n",
    "    # privacy pop-up\n",
    "    xpath = \"/html/body/c-wiz/div/div/div/div[2]/div[1]/div[4]/form/div[1]/div/button/span\"\n",
    "    driver.find_element_by_xpath(xpath).click()\n",
    "\n",
    "    #review_count click\n",
    "    xpath = '//*[@id=\"pane\"]/div/div[1]/div/div/div[2]/div[1]/div[1]/div[2]/div/div[1]/span[1]/span/span[1]/span[2]'\n",
    "\n",
    "    review_count = driver.find_element_by_xpath(xpath).text\n",
    "    review_count=review_count.split(' ', 1)[0]\n",
    "\n",
    "    driver.find_element_by_xpath(xpath).click()\n",
    "\n",
    "    # check\n",
    "    #driver.find_element_by_xpath(\"/html/body/div[3]/div[9]/div[8]/div/div[1]/div/div/div[38]/div/button/span/span\").click()\n",
    "\n",
    "    #scroll to show all reviews\n",
    "    time.sleep(2)\n",
    "    if scroll_limit:\n",
    "        review_count=scroll_limit\n",
    "    scrollable_div = driver.find_element_by_xpath('//*[@id=\"pane\"]/div/div[1]/div/div/div[2]')\n",
    "    for i in range(0,(round(int(review_count)/10-1))):\n",
    "        driver.execute_script('arguments[0].scrollTop = arguments[0].scrollHeight',\n",
    "                scrollable_div)\n",
    "        time.sleep(2)\n",
    "\n",
    "\n",
    "    ### Scrap the reviews info using BS\n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "\n",
    "    #Scrap the reviews text\n",
    "    reviews_soup = soup.find_all('div', class_='ODSEW-ShBeI NIyLF-haAclf gm2-body-2')\n",
    "    reviews = [r.text for r in reviews_soup]\n",
    "\n",
    "    #Scrap the reviews rate\n",
    "    review_rates_soup = [s.find('span',class_='ODSEW-ShBeI-H1e3jb') for s in reviews_soup]\n",
    "    review_rates = [rr.attrs['aria-label'][1] for rr in review_rates_soup]\n",
    "    #Scrap the reviews date\n",
    "    review_dates_soup=[s.find('span', class_='ODSEW-ShBeI-RgZmSc-date') for s in reviews_soup]\n",
    "    review_dates=[rd.text for rd in review_dates_soup]\n",
    "\n",
    "\n",
    "    if return_count:\n",
    "\n",
    "        return review_count,review_dates,review_rates,reviews\n",
    "\n",
    "\n",
    "    return review_dates,review_rates,reviews\n",
    "\n",
    "### Get all reviews from a Google page\n",
    "\n",
    "\n",
    "def get_reviews_google(url,\n",
    "                       scroll_limit=None,\n",
    "                       quiet_mode=True,\n",
    "                       return_count=False):\n",
    "\n",
    "    # Import the webdriver\n",
    "    options = webdriver.ChromeOptions()\n",
    "    options.add_experimental_option('prefs',\n",
    "                                    {'intl.accept_languages': 'en,en_US'})\n",
    "\n",
    "    if quiet_mode:\n",
    "        options.add_argument('--headless')\n",
    "    driver = webdriver.Chrome(chrome_options=options)\n",
    "    driver.get(url)\n",
    "\n",
    "    # privacy pop-up\n",
    "\n",
    "    xpath = \"/html/body/c-wiz/div/div/div/div[2]/div[1]/div[4]/form/div[1]/div/button/span\"\n",
    "    driver.find_element_by_xpath(xpath).click()\n",
    "\n",
    "    #### expand the review\n",
    "\n",
    "    time.sleep(2)\n",
    "\n",
    "    class_ = \"ODSEW-KoToPc-ShBeI gXqMYb-hSRGPd\"\n",
    "\n",
    "    soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "\n",
    "    #total_number_of_reviews = soup.find(\"div\", class_=\"gm2-caption\").text\n",
    "    total_number_of_reviews = driver.find_element_by_xpath(\n",
    "        '//*[@id=\"pane\"]/div/div[1]/div/div/div[2]/div[1]/div[1]/div[2]/div/div[1]/span[1]/span/span[1]/span[2]'\n",
    "    ).text\n",
    "    total_number_of_reviews = total_number_of_reviews.split(' ', 1)[0]\n",
    "\n",
    "    ## Catch nombre d'avis\n",
    "    xpath = '//*[@id=\"pane\"]/div/div[1]/div/div/div[2]/div[1]/div[1]/div[2]/div/div[1]/span[1]/span/span[1]/span[2]'\n",
    "\n",
    "    review_count = driver.find_element_by_xpath(xpath).text\n",
    "    review_count = review_count.split(' ', 1)[0]\n",
    "\n",
    "    driver.find_element_by_xpath(\n",
    "        '//*[@id=\"pane\"]/div/div[1]/div/div/div[2]/div[1]/div[1]/div[2]/div/div[1]/span[1]/span/span[1]/span[2]'\n",
    "    ).click()\n",
    "    #total_number_of_reviews = soup.find(\"div\", class_=\"gm2-caption\").text\n",
    "    #a = total_number_of_reviews\n",
    "    time.sleep(1)\n",
    "    try :\n",
    "        xpatrier = \"/html/body/div[3]/div[9]/div[8]/div/div[1]/div/div/div[2]/div[7]/div[2]/button/span\"\n",
    "        driver.find_element_by_xpath(xpatrier).click()\n",
    "    except :\n",
    "        pass\n",
    "    time.sleep(2)\n",
    "    try :\n",
    "        xpatrecent = \"/html/body/div[3]/div[3]/div[1]/ul/li[2]\"\n",
    "        driver.find_element_by_xpath(xpatrecent).click()\n",
    "    except :\n",
    "        pass\n",
    "    ## Catch cellule of reviews\n",
    "\n",
    "    books_html = soup.findAll('div', class_=\"siAUzd-neVct\")\n",
    "    len(books_html)\n",
    "\n",
    "    #scroll to show all reviews\n",
    "    time.sleep(2)\n",
    "    if scroll_limit:\n",
    "        review_count = scroll_limit\n",
    "    scrollable_div = driver.find_element_by_xpath(\n",
    "        '//*[@id=\"pane\"]/div/div[1]/div/div/div[2]')\n",
    "    for i in range(0, (round(int(review_count) / 10 - 1))):\n",
    "        driver.execute_script(\n",
    "            'arguments[0].scrollTop = arguments[0].scrollHeight',\n",
    "            scrollable_div)\n",
    "        time.sleep(2)\n",
    "\n",
    "    #Find scroll layout\n",
    "    scrollable_div = driver.find_element_by_xpath(\n",
    "        '//*[@id=\"pane\"]/div/div[1]/div/div/div[2]')\n",
    "\n",
    "    print(review_count)\n",
    "    #Scroll as many times as necessary to load all reviews\n",
    "    for i in range(0, (round(int(review_count / 10 - 1)))):\n",
    "        driver.execute_script(\n",
    "            'arguments[0].scrollTop = arguments[0].scrollHeight',\n",
    "            scrollable_div)\n",
    "        time.sleep(2)\n",
    "    plus_list = driver.find_elements_by_link_text(\"Plus\")\n",
    "    for i in plus_list:\n",
    "        i.click()\n",
    "    response = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "\n",
    "    reviews = response.find_all('div',\n",
    "                                class_='ODSEW-ShBeI NIyLF-haAclf gm2-body-2')\n",
    "\n",
    "    return reviews\n",
    "\n",
    "\n",
    "def get_review_summary(result_set):\n",
    "    rev_dict = {'Review Rate': [], 'Review Time': [], 'Review Text': []}\n",
    "    for result in result_set:\n",
    "\n",
    "        review_rate = result.find('span',\n",
    "                                  class_='ODSEW-ShBeI-H1e3jb')[\"aria-label\"]\n",
    "        review_time = result.find('span',\n",
    "                                  class_='ODSEW-ShBeI-RgZmSc-date').text\n",
    "        review_text = result.find('span', class_='ODSEW-ShBeI-text').text\n",
    "        rev_dict['Review Rate'].append(review_rate)\n",
    "        rev_dict['Review Time'].append(review_time)\n",
    "        rev_dict['Review Text'].append(review_text)\n",
    "    A = pd.DataFrame(rev_dict)\n",
    "\n",
    "    A[\"Review Rate\"] = [i.split(\"\\xa0\")[0] for i in A[\"Review Rate\"]]\n",
    "    A[\"Review Time\"] = [i.strip(\"il y a \") for i in A[\"Review Time\"]]\n",
    "    A[\"Review Time\"] = [i.replace(\"une\", \"1\") for i in A[\"Review Time\"]]\n",
    "    A[\"Review Time\"] = [i.replace(\"un\", \"1\") for i in A[\"Review Time\"]]\n",
    "    return A\n",
    "\n",
    "\n",
    "def get_all_gr(url, iid, name, alias):\n",
    "    name\n",
    "    os.path.exists(name.replace(\" \", \"_\").replace(\"'\", \"\") + \".csv\")\n",
    "    if os.path.exists(name.replace(\" \", \"_\").replace(\"'\", \"\") + \".csv\") :\n",
    "        test = get_reviews_google(url, scroll_limit=10, quiet_mode=False)\n",
    "        table = get_review_summary(test)\n",
    "        table[\"id\"] = iid\n",
    "        table[\"name\"] = name\n",
    "        table[\"alias\"] = alias\n",
    "        table.to_csv(name.replace(\" \", \"_\").replace(\"'\", \"\") + \".csv\")\n",
    "        os.system(\"\"\"gsutil cp '*.csv' 'gs://wagon-data-722-manoharan/restaurant/'\"\"\")\n",
    "    else :\n",
    "        test = get_reviews_google(url, scroll_limit=10, quiet_mode=False)\n",
    "        table = get_review_summary(test)\n",
    "        table[\"id\"] = iid\n",
    "        table[\"name\"] = name\n",
    "        table[\"alias\"] = alias\n",
    "        table.to_csv(name.replace(\" \", \"_\").replace(\"'\", \"\") + \".csv\")\n",
    "        os.system(\"\"\"gsutil cp '*.csv' 'gs://wagon-data-722-manoharan/restaurant/'\"\"\")\n",
    "    return True\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b3d37f7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"fobokiller/restaurant_google.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d9079e07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('https://maps.google.com/?cid=4608655436270249005',\n",
       " 'ChIJkXn0aAtv5kcRLaQPm7M79T8',\n",
       " 'La Romantica',\n",
       " 'la-romantica-clichy')"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"lien\"][i],df[\"id\"][i],df[\"name\"][i],df[\"alias\"][i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "7e5533a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/s6/nxj_k6112bj7cnd89_8b9l0c0000gn/T/ipykernel_24950/3032727061.py:262: DeprecationWarning: use options instead of chrome_options\n",
      "  driver = webdriver.Chrome(chrome_options=options)\n",
      "/var/folders/s6/nxj_k6112bj7cnd89_8b9l0c0000gn/T/ipykernel_24950/3032727061.py:268: DeprecationWarning: find_element_by_* commands are deprecated. Please use find_element() instead\n",
      "  driver.find_element_by_xpath(xpath).click()\n",
      "/var/folders/s6/nxj_k6112bj7cnd89_8b9l0c0000gn/T/ipykernel_24950/3032727061.py:279: DeprecationWarning: find_element_by_* commands are deprecated. Please use find_element() instead\n",
      "  total_number_of_reviews = driver.find_element_by_xpath(\n",
      "/var/folders/s6/nxj_k6112bj7cnd89_8b9l0c0000gn/T/ipykernel_24950/3032727061.py:287: DeprecationWarning: find_element_by_* commands are deprecated. Please use find_element() instead\n",
      "  review_count = driver.find_element_by_xpath(xpath).text\n",
      "/var/folders/s6/nxj_k6112bj7cnd89_8b9l0c0000gn/T/ipykernel_24950/3032727061.py:290: DeprecationWarning: find_element_by_* commands are deprecated. Please use find_element() instead\n",
      "  driver.find_element_by_xpath(\n",
      "/var/folders/s6/nxj_k6112bj7cnd89_8b9l0c0000gn/T/ipykernel_24950/3032727061.py:298: DeprecationWarning: find_element_by_* commands are deprecated. Please use find_element() instead\n",
      "  driver.find_element_by_xpath(xpatrier).click()\n",
      "/var/folders/s6/nxj_k6112bj7cnd89_8b9l0c0000gn/T/ipykernel_24950/3032727061.py:304: DeprecationWarning: find_element_by_* commands are deprecated. Please use find_element() instead\n",
      "  driver.find_element_by_xpath(xpatrecent).click()\n",
      "/var/folders/s6/nxj_k6112bj7cnd89_8b9l0c0000gn/T/ipykernel_24950/3032727061.py:316: DeprecationWarning: find_element_by_* commands are deprecated. Please use find_element() instead\n",
      "  scrollable_div = driver.find_element_by_xpath(\n",
      "/var/folders/s6/nxj_k6112bj7cnd89_8b9l0c0000gn/T/ipykernel_24950/3032727061.py:325: DeprecationWarning: find_element_by_* commands are deprecated. Please use find_element() instead\n",
      "  scrollable_div = driver.find_element_by_xpath(\n",
      "/var/folders/s6/nxj_k6112bj7cnd89_8b9l0c0000gn/T/ipykernel_24950/3032727061.py:335: DeprecationWarning: find_elements_by_* commands are deprecated. Please use find_elements() instead\n",
      "  plus_list = driver.find_elements_by_link_text(\"Plus\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying file://Le_Dada.csv [Content-Type=text/csv]...\n",
      "Copying file://Le_Durand_Dupont.csv [Content-Type=text/csv]...                  \n",
      "Copying file://Maison_Rostang.csv [Content-Type=text/csv]...                    \n",
      "Copying file://Dessirier.csv [Content-Type=text/csv]...                         \n",
      "\\ [4 files][207.3 KiB/207.3 KiB]                                                \n",
      "==> NOTE: You are performing a sequence of gsutil operations that may\n",
      "run significantly faster if you instead use gsutil -m cp ... Please\n",
      "see the -m section under \"gsutil help options\" for further information\n",
      "about when gsutil -m can be advantageous.\n",
      "\n",
      "Copying file://Bistrot_du_Passage.csv [Content-Type=text/csv]...\n",
      "Copying file://La_Maison_de_Charly.csv [Content-Type=text/csv]...               \n",
      "Copying file://Le_Petit_Villiers.csv [Content-Type=text/csv]...                 \n",
      "Copying file://LAgapé.csv [Content-Type=text/csv]...                            \n",
      "Copying file://La_Gioconda.csv [Content-Type=text/csv]...                       \n",
      "Copying file://Le_Franc_Tireur.csv [Content-Type=text/csv]...                   \n",
      "Copying file://Le_Dome.csv [Content-Type=text/csv]...                           \n",
      "Copying file://Coretta.csv [Content-Type=text/csv]...                           \n",
      "Copying file://Kiranes.csv [Content-Type=text/csv]...                           \n",
      "Copying file://Trattoria_di_Bellagio.csv [Content-Type=text/csv]...             \n",
      "Copying file://Bouillon_Chartier.csv [Content-Type=text/csv]...                 \n",
      "Copying file://La_Romantica.csv [Content-Type=text/csv]...                      \n",
      "Copying file://Oresto.csv [Content-Type=text/csv]...                            \n",
      "Copying file://Rimal.csv [Content-Type=text/csv]...                             \n",
      "Copying file://restaurant_google.csv [Content-Type=text/csv]...                 \n",
      "Copying file://La_Casa_di_Sergio.csv [Content-Type=text/csv]...                 \n",
      "Copying file://LEntredgeu.csv [Content-Type=text/csv]...                        \n",
      "Copying file://Chartier_Laure_et_Thomas.csv [Content-Type=text/csv]...          \n",
      "Copying file://Schwartzs_Deli.csv [Content-Type=text/csv]...                    \n",
      "Copying file://LAtelier_du_Marché.csv [Content-Type=text/csv]...                \n",
      "Copying file://Le_Comptoir_de_la_Gastronomie.csv [Content-Type=text/csv]...     \n",
      "Copying file://Sébillon.csv [Content-Type=text/csv]...                          \n",
      "Copying file://Il_Etait_une_Oie_dans_le_Sud_Ouest.csv [Content-Type=text/csv]...\n",
      "Copying file://Un_Air_de_famille.csv [Content-Type=text/csv]...                 \n",
      "Copying file://Le_Ballon_des_Ternes.csv [Content-Type=text/csv]...              \n",
      "Copying file://Mum_Dim_Sum.csv [Content-Type=text/csv]...                       \n",
      "Copying file://Angelina.csv [Content-Type=text/csv]...                          \n",
      "Copying file://Le_Chalet_de_Neuilly.csv [Content-Type=text/csv]...              \n",
      "Copying file://Léon_de_Bruxelles.csv [Content-Type=text/csv]...                 \n",
      "| [33 files][  1.3 MiB/  1.3 MiB]   75.4 KiB/s                                  \n",
      "==> NOTE: You are performing a sequence of gsutil operations that may\n",
      "run significantly faster if you instead use gsutil -m cp ... Please\n",
      "see the -m section under \"gsutil help options\" for further information\n",
      "about when gsutil -m can be advantageous.\n",
      "\n",
      "\n",
      "Operation completed over 33 objects/1.3 MiB.                                     \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i = 2\n",
    "get_all_gr(df[\"lien\"][i],df[\"id\"][i],df[\"name\"][i],df[\"alias\"][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "7f23a811",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/s6/nxj_k6112bj7cnd89_8b9l0c0000gn/T/ipykernel_24950/3495851064.py:12: DeprecationWarning: use options instead of chrome_options\n",
      "  driver = webdriver.Chrome(chrome_options=options)\n",
      "/var/folders/s6/nxj_k6112bj7cnd89_8b9l0c0000gn/T/ipykernel_24950/3495851064.py:17: DeprecationWarning: find_element_by_* commands are deprecated. Please use find_element() instead\n",
      "  driver.find_element_by_xpath(xpath).click()\n",
      "/var/folders/s6/nxj_k6112bj7cnd89_8b9l0c0000gn/T/ipykernel_24950/3495851064.py:28: DeprecationWarning: find_element_by_* commands are deprecated. Please use find_element() instead\n",
      "  total_number_of_reviews = driver.find_element_by_xpath('//*[@id=\"pane\"]/div/div[1]/div/div/div[2]/div[1]/div[1]/div[2]/div/div[1]/span[1]/span/span[1]/span[2]').text\n",
      "/var/folders/s6/nxj_k6112bj7cnd89_8b9l0c0000gn/T/ipykernel_24950/3495851064.py:34: DeprecationWarning: find_element_by_* commands are deprecated. Please use find_element() instead\n",
      "  review_count = driver.find_element_by_xpath(xpath).text\n",
      "/var/folders/s6/nxj_k6112bj7cnd89_8b9l0c0000gn/T/ipykernel_24950/3495851064.py:37: DeprecationWarning: find_element_by_* commands are deprecated. Please use find_element() instead\n",
      "  driver.find_element_by_xpath('//*[@id=\"pane\"]/div/div[1]/div/div/div[2]/div[1]/div[1]/div[2]/div/div[1]/span[1]/span/span[1]/span[2]'\n",
      "/var/folders/s6/nxj_k6112bj7cnd89_8b9l0c0000gn/T/ipykernel_24950/3495851064.py:44: DeprecationWarning: find_element_by_* commands are deprecated. Please use find_element() instead\n",
      "  driver.find_element_by_xpath(xpatrier).click()\n",
      "/var/folders/s6/nxj_k6112bj7cnd89_8b9l0c0000gn/T/ipykernel_24950/3495851064.py:50: DeprecationWarning: find_element_by_* commands are deprecated. Please use find_element() instead\n",
      "  driver.find_element_by_xpath(xpatrecent).click()\n",
      "/var/folders/s6/nxj_k6112bj7cnd89_8b9l0c0000gn/T/ipykernel_24950/3495851064.py:54: DeprecationWarning: find_elements_by_* commands are deprecated. Please use find_elements() instead\n",
      "  test = driver.find_elements_by_link_text(\"Plus\")\n",
      "/var/folders/s6/nxj_k6112bj7cnd89_8b9l0c0000gn/T/ipykernel_24950/3495851064.py:62: DeprecationWarning: find_element_by_* commands are deprecated. Please use find_element() instead\n",
      "  scrollable_div = driver.find_element_by_xpath('//*[@id=\"pane\"]/div/div[1]/div/div/div[2]')\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "int() argument must be a string, a bytes-like object or a number, not 'tuple'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/s6/nxj_k6112bj7cnd89_8b9l0c0000gn/T/ipykernel_24950/3495851064.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0mreview_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscroll_limit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0mscrollable_div\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdriver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_element_by_xpath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'//*[@id=\"pane\"]/div/div[1]/div/div/div[2]'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreview_count\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m10\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m     \u001b[0mdriver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute_script\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'arguments[0].scrollTop = arguments[0].scrollHeight'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mscrollable_div\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: int() argument must be a string, a bytes-like object or a number, not 'tuple'"
     ]
    }
   ],
   "source": [
    "url=df[\"lien\"][i]\n",
    "scroll_limit=None,\n",
    "quiet_mode=False,\n",
    "return_count=False\n",
    "\n",
    "# Import the webdriver\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_experimental_option('prefs',\n",
    "                                    {'intl.accept_languages': 'en,en_US'})\n",
    "\n",
    "\n",
    "driver = webdriver.Chrome(chrome_options=options)\n",
    "driver.get(url)\n",
    "\n",
    "    # privacy pop-up\n",
    "xpath = \"/html/body/c-wiz/div/div/div/div[2]/div[1]/div[4]/form/div[1]/div/button/span\"\n",
    "driver.find_element_by_xpath(xpath).click()\n",
    "\n",
    "    #### expand the review\n",
    "\n",
    "time.sleep(2)\n",
    "\n",
    "class_ = \"ODSEW-KoToPc-ShBeI gXqMYb-hSRGPd\"\n",
    "\n",
    "soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "\n",
    "    #total_number_of_reviews = soup.find(\"div\", class_=\"gm2-caption\").text\n",
    "total_number_of_reviews = driver.find_element_by_xpath('//*[@id=\"pane\"]/div/div[1]/div/div/div[2]/div[1]/div[1]/div[2]/div/div[1]/span[1]/span/span[1]/span[2]').text\n",
    "total_number_of_reviews = total_number_of_reviews.split(' ', 1)[0]\n",
    "\n",
    "    ## Catch nombre d'avis\n",
    "xpath = '//*[@id=\"pane\"]/div/div[1]/div/div/div[2]/div[1]/div[1]/div[2]/div/div[1]/span[1]/span/span[1]/span[2]'\n",
    "\n",
    "review_count = driver.find_element_by_xpath(xpath).text\n",
    "review_count = review_count.split(' ', 1)[0]\n",
    "\n",
    "driver.find_element_by_xpath('//*[@id=\"pane\"]/div/div[1]/div/div/div[2]/div[1]/div[1]/div[2]/div/div[1]/span[1]/span/span[1]/span[2]'\n",
    ").click()\n",
    "    #total_number_of_reviews = soup.find(\"div\", class_=\"gm2-caption\").text\n",
    "    #a = total_number_of_reviews\n",
    "time.sleep(1)\n",
    "try :\n",
    "    xpatrier = \"/html/body/div[3]/div[9]/div[8]/div/div[1]/div/div/div[2]/div[7]/div[2]/button/span\"\n",
    "    driver.find_element_by_xpath(xpatrier).click()\n",
    "except :\n",
    "    pass\n",
    "time.sleep(2)\n",
    "try :\n",
    "        xpatrecent = \"/html/body/div[3]/div[3]/div[1]/ul/li[2]\"\n",
    "        driver.find_element_by_xpath(xpatrecent).click()\n",
    "except :\n",
    "    pass\n",
    "    ## Catch cellule of reviews\n",
    "test = driver.find_elements_by_link_text(\"Plus\")\n",
    "books_html = soup.findAll('div', class_=\"siAUzd-neVct\")\n",
    "len(books_html)\n",
    "\n",
    "    #scroll to show all reviews\n",
    "time.sleep(2)\n",
    "if scroll_limit:\n",
    "    review_count = scroll_limit\n",
    "scrollable_div = driver.find_element_by_xpath('//*[@id=\"pane\"]/div/div[1]/div/div/div[2]')\n",
    "for i in range(0, (round(int(review_count) / 10 - 1))):\n",
    "    driver.execute_script('arguments[0].scrollTop = arguments[0].scrollHeight',scrollable_div)\n",
    "    time.sleep(2)\n",
    "\n",
    "    #Find scroll layout\n",
    "scrollable_div = driver.find_element_by_xpath('//*[@id=\"pane\"]/div/div[1]/div/div/div[2]')\n",
    "\n",
    "print(review_count)\n",
    "    #Scroll as many times as necessary to load all reviews\n",
    "for i in range(0, (round(int(review_count / 10 - 1)))):\n",
    "    driver.execute_script('arguments[0].scrollTop = arguments[0].scrollHeight',scrollable_div)\n",
    "    time.sleep(2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "607d9d6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/s6/nxj_k6112bj7cnd89_8b9l0c0000gn/T/ipykernel_24950/3193324684.py:1: DeprecationWarning: find_elements_by_* commands are deprecated. Please use find_elements() instead\n",
      "  driver.find_elements_by_xpath('/html/body/div[3]/div[9]/div[8]/div/div[1]/div/div/div[2]/div[9]/div[79]/div/div[3]/div[3]/jsl/button').click()\n"
     ]
    }
   ],
   "source": [
    "driver.find_elements_by_xpath('/html/body/div[3]/div[9]/div[8]/div/div[1]/div/div/div[2]/div[9]/div[79]/div/div[3]/div[3]/jsl/button').click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "9ebce67c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<selenium.webdriver.chrome.webdriver.WebDriver (session=\"e8a722929147691341a515a22045c1fe\")>"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "439ca93f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'review' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/s6/nxj_k6112bj7cnd89_8b9l0c0000gn/T/ipykernel_24950/2949792035.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmore_button\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf'{review}//a[.=\"(more)\"]'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'review' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "264a0ab9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/s6/nxj_k6112bj7cnd89_8b9l0c0000gn/T/ipykernel_24950/3654066841.py:1: DeprecationWarning: find_elements_by_* commands are deprecated. Please use find_elements() instead\n",
      "  plus_list = driver.find_elements_by_link_text(\"Plus\")\n"
     ]
    }
   ],
   "source": [
    "plus_list = driver.find_elements_by_link_text(\"Plus\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "4ea4331d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plus_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "f7fb3301",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (755780088.py, line 9)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"/var/folders/s6/nxj_k6112bj7cnd89_8b9l0c0000gn/T/ipykernel_24950/755780088.py\"\u001b[0;36m, line \u001b[0;32m9\u001b[0m\n\u001b[0;31m    return reviews\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "plus_list = driver.find_elements_by_link_text(\"Plus\")\n",
    "for i in plus_list:\n",
    "    i.click()\n",
    "response = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "\n",
    "reviews = response.find_all('div',\n",
    "                                class_='ODSEW-ShBeI NIyLF-haAclf gm2-body-2')\n",
    "\n",
    " return reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "067308fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def get_review_summary(result_set):\n",
    "    rev_dict = {'Review Rate': [], 'Review Time': [], 'Review Text': []}\n",
    "    for result in result_set:\n",
    "\n",
    "        review_rate = result.find('span',\n",
    "                                  class_='ODSEW-ShBeI-H1e3jb')[\"aria-label\"]\n",
    "        review_time = result.find('span',\n",
    "                                  class_='ODSEW-ShBeI-RgZmSc-date').text\n",
    "        review_text = result.find('span', class_='ODSEW-ShBeI-text').text\n",
    "        rev_dict['Review Rate'].append(review_rate)\n",
    "        rev_dict['Review Time'].append(review_time)\n",
    "        rev_dict['Review Text'].append(review_text)\n",
    "    A = pd.DataFrame(rev_dict)\n",
    "\n",
    "    A[\"Review Rate\"] = [i.split(\"\\xa0\")[0] for i in A[\"Review Rate\"]]\n",
    "    A[\"Review Time\"] = [i.strip(\"il y a \") for i in A[\"Review Time\"]]\n",
    "    A[\"Review Time\"] = [i.replace(\"une\", \"1\") for i in A[\"Review Time\"]]\n",
    "    A[\"Review Time\"] = [i.replace(\"un\", \"1\") for i in A[\"Review Time\"]]\n",
    "    return A"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
